{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PolInSAR Course - June 3rd, 2024\n",
    "# SAR Polarimetry (PolSAR) \n",
    "# Part 2: Eigenvalues of the Polarimetric Coherency Matrix and the Entropy/Anisotropy/Alpha decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Acquisition: Nkok (Gabon), DLR's F-SAR, L-band\n",
    "\n",
    "* Path to images: /projects/data/polsar/\n",
    "\n",
    "* SLC (single-look complex) images:\n",
    "    * HH: slc_16afrisr0107_Lhh_tcal_test.rat\n",
    "    * HV: slc_16afrisr0107_Lhv_tcal_test.rat\n",
    "    * VH: slc_16afrisr0107_Lvh_tcal_test.rat\n",
    "    * VV: slc_16afrisr0107_Lvv_tcal_test.rat\n",
    "\n",
    "Tips:\n",
    "- use a function that performs the multilook (correlation) operation on a moving window with (looksa x looksr) pixels in range - azimuth\n",
    "- focus on a azimuth - range block within pixels [5000, 15000] and [0, 2000], respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pooch\n",
    "import pooch\n",
    "print(pooch.__version__)\n",
    "\n",
    "!pip install pysarpro\n",
    "import pysarpro\n",
    "print(pysarpro.__version__)\n",
    "\n",
    "!pip install scipy\n",
    "import scipy\n",
    "print(scipy.__version__)\n",
    "\n",
    "!pip install numpy\n",
    "import numpy\n",
    "print(numpy.__version__)\n",
    "\n",
    "!pip install matplotlib\n",
    "import matplotlib\n",
    "print(matplotlib.__version__)\n",
    "!pip install ipympl\n",
    "\n",
    "\n",
    "!pip install cartopy\n",
    "import cartopy\n",
    "print(cartopy.__version__)\n",
    "\n",
    "!pip install pyproj\n",
    "import pyproj\n",
    "print(pyproj.__version__)\n",
    "\n",
    "!pip install pyresample\n",
    "import pyresample\n",
    "print(pyresample.__version__)\n",
    "\n",
    "!pip install rasterio\n",
    "import rasterio\n",
    "print(rasterio.__version__)\n",
    "\n",
    "print(\"the requirments is are satisfied ================================================================= \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Download exercise data & import reader function\n",
    "from pysarpro import io, data\n",
    "from pysarpro.io import rrat\n",
    "\n",
    "#data.download_all(directory=\"/projects\", pattern=r'^data/polsar')\n",
    "\n",
    "# --- Import useful libaries, functions, and modules\n",
    "import sys\n",
    "sys.path.append('/projects/src/')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auxiliary functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HSV_colormap_to_rgb`: Generates and HSV composite representation based on a given colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HSV_colormap_to_rgb(colormap, h, s, v):\n",
    "    \"\"\"\n",
    "    Makes an HSV-like RGB representation based on the given colormap instead\n",
    "    of 'hsv' colormap.\n",
    "    \n",
    "    See https://en.wikipedia.org/wiki/HSL_and_HSV\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    colormap : function\n",
    "        Colormap function. Takes the values in 'h' array and returns an RGBA\n",
    "        value for each point. The ones in matplotlib.cm should be compatible\n",
    "    h : ndarray\n",
    "        Hue values. Usually between 0 and 1.0.\n",
    "    s : ndarray\n",
    "        Saturation values. Between 0 and 1.0.\n",
    "    v : ndarray\n",
    "        Value values. Between 0 and 1.0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rgb: ndarray\n",
    "        An array with the same shape as input + (3,) representing the RGB.\n",
    "    \"\"\"\n",
    "    # Generate color between given colormap (colormap(h)) and white (ones)\n",
    "    # according to the given saturation\n",
    "    tmp = (1-s)[..., np.newaxis]*np.ones(3) + s[..., np.newaxis] * colormap(h)[...,:3]\n",
    "    # Scale it by value\n",
    "    return v[..., np.newaxis] * tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_covariance`: Calculates the covariance between two images while performing a multi-looking operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_covariance(im1, im2, looksa, looksr):\n",
    "    \n",
    "     # ... apply definition\n",
    "    corr = uniform_filter( np.real(im1*np.conj(im2)), [looksa, looksr] ) + \\\n",
    "        1j*uniform_filter( np.imag(im1*np.conj(im2)), [looksa, looksr] )\n",
    "    \n",
    "    # ... and back to main\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_eigenvalues_3`: Computes the eigenvalues of a 3x3 matrix analytically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_eigenvalues_3(T11, T12, T13, T22, T23, T33):\n",
    "\n",
    "    # Calculate and order (from max to min) the eigenvalues of a 3x3 hermitian matrix in closed-form.\n",
    "    # Inputs can be 2D az - rg (rows - columns).\n",
    "\n",
    "    # get dimensions\n",
    "    dims = T11.shape\n",
    "\n",
    "    # calculate auxiliary quantities\n",
    "    A = T11*T22 + T11*T33 + T22*T33 - T12*np.conj(T12) - T13*np.conj(T13) - T23*np.conj(T23)\n",
    "    B = T11**2 - T11*T22 + T22**2 -T11*T33 -T22*T33 + T33**2 + 3*T12*np.conj(T12) + 3*T13*np.conj(T13) + 3*T23*np.conj(T23)\n",
    "\n",
    "    DET = T11*T22*T33 - T33*T12*np.conj(T12) - T22*T13*np.conj(T13) - T11*T23*np.conj(T23) + T12*np.conj(T13)*T23 + np.conj(T12)*T13*np.conj(T23)  \n",
    "    TR = T11 + T22 + T33 \n",
    "    Z = 27*DET-9*A*TR + 2*TR**3 + np.sqrt((27*DET-9*A*TR + 2*TR**3)**2-4*B**3)\n",
    "    \n",
    "    del DET\n",
    "    \n",
    "    # ... and here they are:\n",
    "    LA = ( 1/3.*TR + 2**(1/3.)*B/(3*Z**(1/3.)) + Z**(1/3.)/(3*2**(1/3.)) )\n",
    "    LB = ( 1/3.*TR - (1+1j*np.sqrt(3))*B/(3*2**(2/3.)*Z**(1/3.)) - (1-1j*np.sqrt(3))*Z**(1/3.)/(6*2**(1/3.)) )\n",
    "    LC = ( 1/3.*TR - (1-1j*np.sqrt(3))*B/(3*2**(2/3.)*Z**(1/3.)) - (1+1j*np.sqrt(3))*Z**(1/3.)/(6*2**(1/3.)) )\n",
    "    \n",
    "    # now order them:\n",
    "    dumm = np.zeros((dims[0], dims[1], 3), 'float32')\n",
    "    dumm [:, :, 0] = np.real(LA)\n",
    "    dumm [:, :, 1] = np.real(LB)\n",
    "    dumm [:, :, 2] = np.real(LC)\n",
    "    \n",
    "    del LA, LB, LC  \n",
    "    \n",
    "    L1 = np.max(dumm, axis = 2)\n",
    "    L3 = np.min(dumm, axis = 2)\n",
    "    L2 = np.sum(dumm, axis = 2) - L1 - L3\n",
    "    \n",
    "    del dumm\n",
    "    \n",
    "    return L1, L2, L3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_eigenvectors_3`: Computes the eigenvectors of a 3x3 matrix analytically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_eigenvectors_3(T11, T12, T13, T22, T23, T33, L1, L2, L3) :\n",
    "\n",
    "    # Calculate the eigenvectors corresponding to the eigenvalues (L1, L2, L3)\n",
    "    # of a 3x3 matrix \n",
    "    # Inputs can be 2D az - rg (rows - columns).\n",
    "\n",
    "    # get dimensions\n",
    "    dims = T11.shape    \n",
    "    \n",
    "    # first eigenvector - corresponds to the maximum eigenvalue L1\n",
    "    U1 = np.ones((dims[0], dims[1], 3), 'complex64')\n",
    "    U1[:, :, 0] = (L1 -T33)/np.conj(T13) + (((L1-T33)*np.conj(T12) + np.conj(T13)*T23)*np.conj(T23))/ \\\n",
    "                    (((T22-L1)*np.conj(T13) - np.conj(T12)*np.conj(T23))*np.conj(T13))\n",
    "    U1[:, :, 1] = -((L1-T33)*np.conj(T12)+np.conj(T13)*T23) / ((T22-L1)*np.conj(T13) - np.conj(T12)*np.conj(T23))\n",
    "    \n",
    "    # second eigenvector - corresponds to the eigenvalue L2\n",
    "    U2 = np.ones((dims[0], dims[1], 3), 'complex64')\n",
    "    U2[:, :, 0] = (L2 -T33)/np.conj(T13) + (((L2-T33)*np.conj(T12) + np.conj(T13)*T23)*np.conj(T23))/ \\\n",
    "                    (((T22-L2)*np.conj(T13) - np.conj(T12)*np.conj(T23))*np.conj(T13))\n",
    "    U2[:, :, 1] = -((L2-T33)*np.conj(T12)+np.conj(T13)*T23) / ((T22-L2)*np.conj(T13) - np.conj(T12)*np.conj(T23))\n",
    "    \n",
    "    # third eigenvector - corresponds to the minimum eigenvalue L3\n",
    "    U3 = np.ones((dims[0], dims[1], 3), 'complex64')\n",
    "    U3[:, :, 0] = (L3 -T33)/np.conj(T13) + (((L3-T33)*np.conj(T12) + np.conj(T13)*T23)*np.conj(T23))/ \\\n",
    "                    (((T22-L3)*np.conj(T13) - np.conj(T12)*np.conj(T23))*np.conj(T13))\n",
    "    U3[:, :, 1] = -((L3-T33)*np.conj(T12)+np.conj(T13)*T23) / ((T22-L3)*np.conj(T13) - np.conj(T12)*np.conj(T23))   \n",
    "    \n",
    "    # normalize to get orthonormal eigenvectors\n",
    "    norm1 = np.sqrt( np.abs(U1[:,:,0])**2 + np.abs(U1[:,:,1])**2 + np.abs(U1[:,:,2])**2)\n",
    "    norm2 = np.sqrt( np.abs(U2[:,:,0])**2 + np.abs(U2[:,:,1])**2 + np.abs(U2[:,:,2])**2)    \n",
    "    norm3 = np.sqrt( np.abs(U3[:,:,0])**2 + np.abs(U3[:,:,1])**2 + np.abs(U3[:,:,2])**2)        \n",
    "    for nn in range(3):\n",
    "        U1[:,:,nn] = U1[:,:,nn] / norm1\n",
    "        U2[:,:,nn] = U2[:,:,nn] / norm2\n",
    "        U3[:,:,nn] = U3[:,:,nn] / norm3\n",
    "        \n",
    "    del norm1, norm2, norm3     \n",
    "    \n",
    "    return U1, U2, U3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to the data\n",
    "path = '/projects/data/polsar/'\n",
    "# define the number of looks \n",
    "looksa = 7\n",
    "looksr = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slcHH = rrat(path + 'slc_16afrisr0107_Lhh_tcal_test.rat', block = [5000, 15000, 0, 2000])\n",
    "slcVV = rrat(path + 'slc_16afrisr0107_Lvv_tcal_test.rat', block = [5000, 15000, 0, 2000])\n",
    "slcHV = rrat(path + 'slc_16afrisr0107_Lhv_tcal_test.rat', block = [5000, 15000, 0, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "slcHH.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Calculate the necessary elements of the coherency matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- compute the Pauli components\n",
    "pauli1 = slcHH + slcVV\n",
    "pauli2 = slcHH - slcVV\n",
    "pauli3 = 2*slcHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- compute the elements of the coherency matrix\n",
    "T11 = calculate_covariance(pauli1, pauli1, looksa, looksr)\n",
    "T22 = calculate_covariance(pauli2, pauli2, looksa, looksr)\n",
    "T33 = calculate_covariance(pauli3, pauli3, looksa, looksr)\n",
    "T12 = calculate_covariance(pauli1, pauli2, looksa, looksr)\n",
    "T13 = calculate_covariance(pauli1, pauli3, looksa, looksr)\n",
    "T23 = calculate_covariance(pauli2, pauli3, looksa, looksr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- delete unused variables\n",
    "del slcHH, slcVV, slcHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pauli1, pauli2, pauli3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Calculate eigenvalues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambda1, lambda2, lambda3 = calculate_eigenvalues_3(T11, T12, T13, T22, T23, T33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "lambda1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Calculate entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- compute the probabilities associated with each eigenvalue\n",
    "pr1 = lambda1 / (lambda1 + lambda2 + lambda3)\n",
    "pr2 = lambda2 / (lambda1 + lambda2 + lambda3)\n",
    "pr3 = lambda3 / (lambda1 + lambda2 + lambda3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- compute the entropy\n",
    "entropy = -(pr1*np.log10(pr1)/np.log10(3) + pr2*np.log10(pr2)/np.log10(3) + pr3*np.log10(pr3)/np.log10(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Calculate anisotropy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- compute the anisotropy (related to the minimum and intermediate eigenvalues)\n",
    "# A = 0 when lambda2 = lambda3\n",
    "# A = 1 when lambda2 >> lambda3 \n",
    "anisotropy = (lambda2 - lambda3) / (lambda2 + lambda3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Calculate eigenvectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- compute the eigenvectors\n",
    "U1, U2, U3 = calculate_eigenvectors_3(T11, T12, T13, T22, T23, T33, lambda1, lambda2, lambda3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "U1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- delete unused variables\n",
    "del T12, T23, T13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Calculate mean alpha angle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- extract the alpha angles\n",
    "alpha1 = np.arccos(abs(U1[:,:,0])) # [rad]\n",
    "alpha2 = np.arccos(abs(U2[:,:,0]))\n",
    "alpha3 = np.arccos(abs(U3[:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- delete unused variables\n",
    "del U1, U2, U3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- compute the mean alpha angle\n",
    "alpha_mean = alpha1*pr1 + alpha2*pr2 + alpha3*pr3 # [rad]\n",
    "alpha_mean = np.degrees(alpha_mean) # [deg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Plots!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculations for Paulis RGB:\n",
    "# -- define the 3D array for the Pauli representation\n",
    "dimaz = lambda1.shape[0]\n",
    "dimrg = lambda1.shape[1]\n",
    "rgb_pauli = np.zeros((dimrg, dimaz, 3), 'float32')\n",
    "# -- fill the array, clipping the values between 0 and 2.5xmean(amplitude)\n",
    "rgb_pauli[:,:,0] = np.clip(np.transpose(np.sqrt(abs(T22))), 0,2.5*np.mean(np.sqrt(abs(T22)))) # R : HH-VV \n",
    "rgb_pauli[:,:,1] = np.clip(np.transpose(np.sqrt(abs(T33))), 0,2.5*np.mean(np.sqrt(abs(T33)))) #  G : HV\n",
    "rgb_pauli[:,:,2] = np.clip(np.transpose(np.sqrt(abs(T11))), 0,2.5*np.mean(np.sqrt(abs(T11)))) #  G : HV\n",
    "# -- normalisation: values between 0 and 1\n",
    "rgb_pauli[:,:,0] = rgb_pauli[:,:,0] / np.max(rgb_pauli[:,:,0])\n",
    "rgb_pauli[:,:,1] = rgb_pauli[:,:,1] / np.max(rgb_pauli[:,:,1])\n",
    "rgb_pauli[:,:,2] = rgb_pauli[:,:,2] / np.max(rgb_pauli[:,:,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot: Pauli RGB and eigenvalue probabilities\n",
    "\n",
    "# plt.figure(figsize=(15, 6*4))\n",
    "# plt.subplot(4,1,1)\n",
    "# plt.imshow(rgb_pauli, aspect = 'auto')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.subplot(4,1,2)\n",
    "# plt.imshow(np.transpose(pr1), cmap = 'turbo', vmin =0, vmax=1,aspect = 'auto')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.subplot(4,1,3)\n",
    "# plt.imshow(np.transpose(pr2), cmap = 'turbo', vmin =0, vmax=1,aspect = 'auto')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.subplot(4,1,4)\n",
    "# plt.imshow(np.transpose(pr3), cmap = 'turbo', vmin =0, vmax=1,aspect = 'auto')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot: H, A, alpha\n",
    "\n",
    "# plt.figure(figsize= (15, 6*3))\n",
    "# plt.subplot(3,1,1)\n",
    "# plt.imshow(np.transpose(entropy), cmap = 'gray', vmin = 0, vmax =1, aspect = 'auto')\n",
    "# cb = plt.colorbar()\n",
    "# cb.set_label('H')\n",
    "\n",
    "# plt.subplot(3,1,2)\n",
    "# plt.imshow(np.transpose(anisotropy), cmap = 'turbo', vmin = 0, vmax =1, aspect = 'auto')\n",
    "# cb = plt.colorbar()\n",
    "# cb.set_label('A')\n",
    "\n",
    "# plt.subplot(3,1,3)\n",
    "# plt.imshow(np.transpose(alpha_mean), cmap = 'turbo', vmin = 0, vmax = 90, aspect = 'auto')\n",
    "# cb = plt.colorbar()\n",
    "# cb.set_label('mean alpha [deg]')\n",
    "\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HSI Color Representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " HSI Color Representation:\n",
    "- H (hue):  mean alpha angle\n",
    "- S (saturation): \n",
    "     - Case 1: saturation = 1: always full colorscale\n",
    "     - Case 2:  saturation = 1 - entropy\n",
    "          - when entropy = 0: then saturation = 1: full colorscale\n",
    "          - when entropy = 1: then saturation = 0: grayscale\n",
    "- I (intensity): amplitude of total power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hue: mean alpha angle\n",
    "# normalize the mean alpha angle: it has to be between 0 and 1 --> divide by 90 degrees\n",
    "alpha_mean = alpha_mean / 90\n",
    "\n",
    "# Import the colormap for plotting alpha\n",
    "colormap = plt.colormaps.get('turbo')\n",
    "\n",
    "# Intensity: normalize the amplitude\n",
    "amp = np.sqrt(abs(T11) + abs(T22) + abs(T33))\n",
    "amp = np.clip(amp, 0, 2.5*np.mean(amp))\n",
    "amp = amp / np.max(amp)\n",
    "\n",
    "# Saturation\n",
    "# Case 1)\n",
    "sat1 = np.ones_like(amp)\n",
    "# Case 2)\n",
    "sat2 = 1 - entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the HSV colormaps \n",
    "\n",
    "# Case 1\n",
    "hsv_comp1 = HSV_colormap_to_rgb(colormap, alpha_mean, sat1, amp)\n",
    "\n",
    "# Case 2\n",
    "hsv_comp2 = HSV_colormap_to_rgb(colormap, alpha_mean, sat2, amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- delete unused variables\n",
    "del amp, sat1, sat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- Plot: HSI representations\n",
    "\n",
    "# plt.figure(figsize = (12,12))\n",
    "\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.imshow(np.transpose(hsv_comp1, axes = (1,0,2)) , aspect = 'auto')\n",
    "\n",
    "# plt.subplot(2,1,2)\n",
    "# plt.imshow(np.transpose(hsv_comp2, axes = (1,0,2)) , aspect = 'auto')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™ðŸ“™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirments \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTModel, ViTFeatureExtractor, pipeline\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # works fine \n",
    "\n",
    "\n",
    "# # Plot each parameter individually\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# # Alpha Mean (degrees)\n",
    "# im1 = axes[0].imshow(alpha_mean, cmap='turbo', vmin=0, vmax=90)\n",
    "# axes[0].set_title('Alpha Mean [Â°]')\n",
    "# plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# # Entropy (0 to 1)\n",
    "# im2 = axes[1].imshow(entropy, cmap='viridis', vmin=0, vmax=1)\n",
    "# axes[1].set_title('Entropy')\n",
    "# plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# # Anisotropy (0 to 1)\n",
    "# im3 = axes[2].imshow(anisotropy, cmap='plasma', vmin=0, vmax=1)\n",
    "# axes[2].set_title('Anisotropy')\n",
    "# plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, import all required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTModel  # Using ViTImageProcessor instead of deprecated ViTFeatureExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, widgets\n",
    "from IPython.display import display\n",
    "from transformers import pipeline\n",
    "\n",
    "# 2. Load your SAR data (replace with your actual data loading)\n",
    "# Assuming you already have these arrays from your previous processing:\n",
    "# alpha_mean, entropy, anisotropy\n",
    "\n",
    "# 3. Stack and normalize the layers (your existing code)\n",
    "sar_features = np.stack([alpha_mean, entropy, anisotropy], axis=-1)\n",
    "sar_features = (sar_features - sar_features.min()) / (sar_features.max() - sar_features.min())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Convert to PyTorch tensor (THIS MUST COME BEFORE THE ViT CODE)\n",
    "sar_tensor = torch.from_numpy(sar_features).permute(2, 0, 1).float()  # Shape: [3, H, W]\n",
    "\n",
    "# 5. Now initialize ViT model\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# 6. Extract patches\n",
    "inputs = processor(images=sar_tensor.unsqueeze(0), return_tensors=\"pt\")  # Now sar_tensor is defined\n",
    "with torch.no_grad():\n",
    "    outputs = vit_model(**inputs)\n",
    "patches = outputs.last_hidden_state  # Shape: [1, 197, 768]\n",
    "\n",
    "print(\"Successfully extracted patches!\")\n",
    "print(f\"Patch tensor shape: {patches.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "## just for checking and testing \n",
    "# Add these right after tensor conversion\n",
    "print(\"\\n=== Data Shape Verification ===\")\n",
    "print(f\"Original shapes: {alpha_mean.shape}, {entropy.shape}, {anisotropy.shape}\")\n",
    "print(f\"Stacked shape: {sar_features.shape}\")\n",
    "print(f\"Tensor shape: {sar_tensor.shape}\")\n",
    "\n",
    "# Verify normalization\n",
    "print(\"\\n=== Value Ranges ===\")\n",
    "print(f\"Stacked min/max: {sar_features.min():.2f}, {sar_features.max():.2f}\")\n",
    "print(f\"Tensor min/max: {sar_tensor.min():.2f}, {sar_tensor.max():.2f}\")\n",
    "\n",
    "# ViT input checks\n",
    "print(\"\\n=== ViT Input Checks ===\")\n",
    "print(f\"Processor default size: {processor.size}\")\n",
    "print(f\"Input tensor shape to ViT: {inputs['pixel_values'].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Visualize stacked image BEFORE tensor conversion\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(sar_features)  # Auto-displays first 3 channels as RGB\n",
    "plt.title(\"Stacked SAR Features (HxWx3)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# 2. Visualize AFTER tensor conversion (per-channel)\n",
    "plt.subplot(122)\n",
    "# Convert back to HxWxC for visualization\n",
    "tensor_vis = sar_tensor.permute(1, 2, 0).numpy()\n",
    "plt.imshow(tensor_vis)\n",
    "plt.title(\"Tensor Format (3xHxW -> HxWx3)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_scattering(alpha, entropy, anisotropy):\n",
    "#     if entropy < 0.3:\n",
    "#         if alpha < 30: return \"Z1: Low Entropy Surface Scatter\"\n",
    "#         elif 30 <= alpha <= 50: return \"Z2: Low Entropy Dipole Scatter\"\n",
    "#         else: return \"Z3: Low Entropy Multiple Scatter\"\n",
    "#     elif 0.3 <= entropy < 0.7:\n",
    "#         if alpha < 30: return \"Z4: Medium Entropy Surface Scatter\"\n",
    "#         elif 30 <= alpha <= 50: return \"Z5: Medium Entropy Dipole Scatter\"\n",
    "#         else: return \"Z6: Medium Entropy Multiple Scatter\"\n",
    "#     else:\n",
    "#         if alpha < 30: return \"Z7: High Entropy Surface Scatter\"\n",
    "#         elif 30 <= alpha <= 50: return \"Z8: High Entropy Dipole Scatter\"\n",
    "#         else: return \"Z9: High Entropy Multiple Scatter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from ipywidgets import interact, interactive, fixed, widgets\n",
    "# from IPython.display import display\n",
    "# from transformers import pipeline  # NEW: For VQA\n",
    "\n",
    "# # Your SAR data (example arrays - replace with your actual data)\n",
    "# alpha_mean = np.random.rand(1000, 1000) * 90  # 0-90 degrees\n",
    "# entropy = np.random.rand(1000, 1000)          # 0-1\n",
    "# anisotropy = np.random.rand(1000, 1000)       # 0-1\n",
    "# sar_features = np.stack([alpha_mean, entropy, anisotropy], axis=-1)\n",
    "\n",
    "# def create_sar_composite(alpha, entropy, anisotropy):\n",
    "#     # Normalize each channel with enhanced contrast\n",
    "#     r = np.clip((alpha - np.percentile(alpha, 5)) / (np.percentile(alpha, 95) - np.percentile(alpha, 5)), 0, 1)\n",
    "#     g = np.clip((entropy - np.percentile(entropy, 5)) / (np.percentile(entropy, 95) - np.percentile(entropy, 5)), 0, 1)\n",
    "#     b = np.clip((anisotropy - np.percentile(anisotropy, 5)) / (np.percentile(anisotropy, 95) - np.percentile(anisotropy, 5)), 0, 1)\n",
    "    \n",
    "#     # Apply gamma correction for better visual contrast\n",
    "#     composite = np.stack([\n",
    "#         np.power(r, 0.6),  # Red channel (alpha)\n",
    "#         np.power(g, 0.7),  # Green channel (entropy)\n",
    "#         np.power(b, 0.8)   # Blue channel (anisotropy)\n",
    "#     ], axis=-1)\n",
    "    \n",
    "#     return np.clip(composite, 0, 1)\n",
    "\n",
    "# # Create the enhanced composite\n",
    "# sar_composite = create_sar_composite(alpha_mean, entropy, anisotropy)\n",
    "\n",
    "# # Display with proper scaling\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(sar_composite, aspect='auto')\n",
    "# plt.colorbar(label='Normalized Intensity')\n",
    "# plt.title('Enhanced SAR Composite (Red=Alpha, Green=Entropy, Blue=Anisotropy)')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # NEW: Precompute patches for faster interaction\n",
    "# def precompute_patches(feature_map, patch_size=16):\n",
    "#     return np.lib.stride_tricks.sliding_window_view(\n",
    "#         feature_map, (patch_size, patch_size)\n",
    "#     )[::patch_size, ::patch_size].mean(axis=(2,3))\n",
    "\n",
    "# alpha_patches = precompute_patches(alpha_mean)  # NEW\n",
    "# entropy_patches = precompute_patches(entropy)   # NEW\n",
    "# anisotropy_patches = precompute_patches(anisotropy)  # NEW\n",
    "\n",
    "# # NEW: Initialize VQA pipeline\n",
    "# qa_pipeline = pipeline(\"question-answering\", \n",
    "#                       model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# def classify_scattering(alpha, entropy, anisotropy):\n",
    "#     \"\"\"Classify scattering mechanism into zones\"\"\"\n",
    "#     if entropy < 0.3:\n",
    "#         if alpha < 30: return \"Z1: Low Entropy Surface Scatter\"\n",
    "#         elif 30 <= alpha <= 50: return \"Z2: Low Entropy Dipole Scatter\"\n",
    "#         else: return \"Z3: Low Entropy Multiple Scatter\"\n",
    "#     elif 0.3 <= entropy < 0.7:\n",
    "#         if alpha < 30: return \"Z4: Medium Entropy Surface Scatter\"\n",
    "#         elif 30 <= alpha <= 50: return \"Z5: Medium Entropy Dipole Scatter\"\n",
    "#         else: return \"Z6: Medium Entropy Multiple Scatter\"\n",
    "#     else:\n",
    "#         if alpha < 30: return \"Z7: High Entropy Surface Scatter\"\n",
    "#         elif 30 <= alpha <= 50: return \"Z8: High Entropy Dipole Scatter\"\n",
    "#         else: return \"Z9: High Entropy Multiple Scatter\"\n",
    "\n",
    "\n",
    "# def analyze_region(x=0, y=0, question=\"\"):\n",
    "#     patch_size = 16\n",
    "#     x, y = min(x, alpha_mean.shape[1]-16), min(y, alpha_mean.shape[0]-16)\n",
    "    \n",
    "#     # Use precomputed patches (faster)\n",
    "#     patch_x, patch_y = x//16, y//16\n",
    "#     patch_alpha = alpha_patches[patch_y, patch_x]\n",
    "#     patch_entropy = entropy_patches[patch_y, patch_x]\n",
    "#     patch_anisotropy = anisotropy_patches[patch_y, patch_x]\n",
    "    \n",
    "#     zone = classify_scattering(patch_alpha, patch_entropy, patch_anisotropy)\n",
    "    \n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,5))\n",
    "    \n",
    "#     # 1. Actual SAR patch with red border\n",
    "#     ax1.imshow(sar_composite[y:y+patch_size, x:x+patch_size])\n",
    "#     rect = plt.Rectangle(\n",
    "#         (0, 0),           # Bottom-left corner (relative to patch)\n",
    "#         patch_size,        # Width\n",
    "#         patch_size,        # Height\n",
    "#         linewidth=2,       # Border thickness\n",
    "#         edgecolor='red',   # Border color\n",
    "#         facecolor='none'   # Transparent fill\n",
    "#     )\n",
    "#     ax1.add_patch(rect)\n",
    "#     ax1.set_title(f\"SAR Patch ({x}-{x+patch_size}, {y}-{y+patch_size})\")\n",
    "    \n",
    "#     # 2. Text with VQA\n",
    "#     context = f\"{zone}\\nAlpha: {patch_alpha:.1f}Â°\\nEntropy: {patch_entropy:.2f}\\nAnisotropy: {patch_anisotropy:.2f}\"\n",
    "#     if question:\n",
    "#         answer = qa_pipeline(question=question, context=context)\n",
    "#         context += f\"\\n\\nQ: {question}\\nA: {answer['answer']}\"\n",
    "#     ax2.text(0.5, 0.5, context, ha='center', va='center', fontsize=10)\n",
    "#     ax2.axis('off')\n",
    "    \n",
    "#     # 3. Enhanced H/Alpha plot\n",
    "#     ax3.scatter(patch_alpha, patch_entropy, c='red', s=100)\n",
    "#     ax3.set_xlim(0, 90); ax3.set_ylim(0, 1)\n",
    "#     ax3.set_xlabel(\"Alpha Angle (Â°)\"); ax3.set_ylabel(\"Entropy\")\n",
    "#     ax3.axhline(0.3, color='red', linestyle='--', alpha=0.5)\n",
    "#     ax3.axhline(0.7, color='red', linestyle='--', alpha=0.5)\n",
    "#     ax3.axvline(30, color='blue', linestyle='--', alpha=0.5)\n",
    "#     ax3.axvline(50, color='blue', linestyle='--', alpha=0.5)\n",
    "#     ax3.grid(True)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     return zone\n",
    "\n",
    "# # Create interactive widgets (unchanged)\n",
    "# x_slider = widgets.IntSlider(\n",
    "#     min=0, \n",
    "#     max=alpha_mean.shape[1]-16, \n",
    "#     step=16, \n",
    "#     value=0,\n",
    "#     description='X Position:'\n",
    "# )\n",
    "# y_slider = widgets.IntSlider(\n",
    "#     min=0, \n",
    "#     max=alpha_mean.shape[0]-16, \n",
    "#     step=16, \n",
    "#     value=0,\n",
    "#     description='Y Position:'\n",
    "# )\n",
    "# question_box = widgets.Text(\n",
    "#     placeholder='Ask about this region...',\n",
    "#     description='Question:'\n",
    "# )\n",
    "\n",
    "# # Create interactive UI (unchanged)\n",
    "# ui = widgets.VBox([\n",
    "#     widgets.HBox([x_slider, y_slider]),\n",
    "#     question_box\n",
    "# ])\n",
    "\n",
    "# out = widgets.interactive_output(\n",
    "#     analyze_region,\n",
    "#     {'x': x_slider, 'y': y_slider, 'question': question_box}\n",
    "# )\n",
    "\n",
    "# display(ui, out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import RectangleSelector\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensure we're using the correct backend for interactivity in Jupyter\n",
    "%matplotlib widget\n",
    "\n",
    "# Precompute patches if not already done\n",
    "def ensure_valid_dimensions(feature_map, patch_size=16):\n",
    "    # Get current dimensions\n",
    "    h, w = feature_map.shape\n",
    "    \n",
    "    # Calculate padding needed\n",
    "    pad_h = (patch_size - h % patch_size) % patch_size\n",
    "    pad_w = (patch_size - w % patch_size) % patch_size\n",
    "    \n",
    "    # Pad if needed\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        return np.pad(feature_map, ((0, pad_h), (0, pad_w)), mode='constant')\n",
    "    return feature_map\n",
    "\n",
    "def precompute_patches(feature_map, patch_size=16):\n",
    "    padded_map = ensure_valid_dimensions(feature_map, patch_size)\n",
    "    return np.lib.stride_tricks.sliding_window_view(\n",
    "        padded_map, (patch_size, patch_size)\n",
    "    )[::patch_size, ::patch_size].mean(axis=(2,3))\n",
    "\n",
    "# Create SAR composite if not already done\n",
    "def create_sar_composite(alpha, entropy, anisotropy):\n",
    "    # Normalize each channel with enhanced contrast\n",
    "    r = np.clip((alpha - np.percentile(alpha, 5)) / (np.percentile(alpha, 95) - np.percentile(alpha, 5)), 0, 1)\n",
    "    g = np.clip((entropy - np.percentile(entropy, 5)) / (np.percentile(entropy, 95) - np.percentile(entropy, 5)), 0, 1)\n",
    "    b = np.clip((anisotropy - np.percentile(anisotropy, 5)) / (np.percentile(anisotropy, 95) - np.percentile(anisotropy, 5)), 0, 1)\n",
    "    \n",
    "    # Apply gamma correction for better visual contrast\n",
    "    composite = np.stack([\n",
    "        np.power(r, 0.6),  # Red channel (alpha)\n",
    "        np.power(g, 0.7),  # Green channel (entropy)\n",
    "        np.power(b, 0.8)   # Blue channel (anisotropy)\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return np.clip(composite, 0, 1)\n",
    "\n",
    "# Precompute patches\n",
    "alpha_patches = precompute_patches(alpha_mean)\n",
    "entropy_patches = precompute_patches(entropy)\n",
    "anisotropy_patches = precompute_patches(anisotropy)\n",
    "\n",
    "# Create composite\n",
    "sar_composite = create_sar_composite(alpha_mean, entropy, anisotropy)\n",
    "\n",
    "# Initialize QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Set up classification function\n",
    "def classify_scattering(alpha, entropy, anisotropy):\n",
    "    \"\"\"Classify scattering mechanism into zones\"\"\"\n",
    "    if entropy < 0.3:\n",
    "        if alpha < 30: return \"Z1: Low Entropy Surface Scatter\"\n",
    "        elif 30 <= alpha <= 50: return \"Z2: Low Entropy Dipole Scatter\"\n",
    "        else: return \"Z3: Low Entropy Multiple Scatter\"\n",
    "    elif 0.3 <= entropy < 0.7:\n",
    "        if alpha < 30: return \"Z4: Medium Entropy Surface Scatter\"\n",
    "        elif 30 <= alpha <= 50: return \"Z5: Medium Entropy Dipole Scatter\"\n",
    "        else: return \"Z6: Medium Entropy Multiple Scatter\"\n",
    "    else:\n",
    "        if alpha < 30: return \"Z7: High Entropy Surface Scatter\"\n",
    "        elif 30 <= alpha <= 50: return \"Z8: High Entropy Dipole Scatter\"\n",
    "        else: return \"Z9: High Entropy Multiple Scatter\"\n",
    "\n",
    "# Create a text widget for questions\n",
    "question_input = widgets.Text(\n",
    "    placeholder='Ask about this region...',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create an output widget for the analysis results\n",
    "analysis_output = widgets.Output()\n",
    "\n",
    "# Interactive visualization system\n",
    "class SARImageAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.patch_size = 16\n",
    "        self.selected_x = 0\n",
    "        self.selected_y = 0\n",
    "        self.rect = None\n",
    "        self.setup_ui()\n",
    "        \n",
    "    def setup_ui(self):\n",
    "        # Create main figure for the composite image (larger size)\n",
    "        self.fig, self.ax = plt.subplots(figsize=(12, 12))\n",
    "        self.img = self.ax.imshow(sar_composite, aspect='auto')\n",
    "        plt.colorbar(self.img, label='Normalized Intensity')\n",
    "        self.ax.set_title('SAR Composite (Red=Alpha, Green=Entropy, Blue=Anisotropy)\\nClick to select a region')\n",
    "        \n",
    "        # Connect the click event\n",
    "        self.fig.canvas.mpl_connect('button_press_event', self.on_click)\n",
    "    \n",
    "    def on_click(self, event):\n",
    "        if event.inaxes != self.ax:\n",
    "            return\n",
    "            \n",
    "        # Get clicked coordinates (rounded to nearest patch boundary)\n",
    "        self.selected_x = int(event.xdata // self.patch_size * self.patch_size)\n",
    "        self.selected_y = int(event.ydata // self.patch_size * self.patch_size)\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        self.selected_x = min(self.selected_x, sar_composite.shape[1] - self.patch_size)\n",
    "        self.selected_y = min(self.selected_y, sar_composite.shape[0] - self.patch_size)\n",
    "        \n",
    "        # Highlight selection with rectangle\n",
    "        if self.rect:\n",
    "            self.rect.remove()\n",
    "        self.rect = plt.Rectangle(\n",
    "            (self.selected_x, self.selected_y),\n",
    "            self.patch_size, self.patch_size,\n",
    "            linewidth=3, edgecolor='white', facecolor='none'\n",
    "        )\n",
    "        self.ax.add_artist(self.rect)\n",
    "        self.fig.canvas.draw_idle()\n",
    "        \n",
    "        # Update analysis (this will use the question from the input widget)\n",
    "        with analysis_output:\n",
    "            clear_output(wait=True)\n",
    "            self.analyze_region(question_input.value)\n",
    "    \n",
    "    def analyze_region(self, question=\"\"):\n",
    "        patch_x, patch_y = self.selected_x // self.patch_size, self.selected_y // self.patch_size\n",
    "        patch_alpha = alpha_patches[patch_y, patch_x]\n",
    "        patch_entropy = entropy_patches[patch_y, patch_x]\n",
    "        patch_anisotropy = anisotropy_patches[patch_y, patch_x]\n",
    "        \n",
    "        zone = classify_scattering(patch_alpha, patch_entropy, patch_anisotropy)\n",
    "        \n",
    "        # Create analysis figure (bigger than before)\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "        \n",
    "        # 1. SAR patch with red border\n",
    "        ax1.imshow(sar_composite[self.selected_y:self.selected_y+self.patch_size, \n",
    "                                 self.selected_x:self.selected_x+self.patch_size], \n",
    "                   interpolation='bilinear')  # Added interpolation for smoother appearance\n",
    "        rect = plt.Rectangle(\n",
    "            (0, 0),\n",
    "            self.patch_size, self.patch_size,\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.set_title(f\"SAR Patch ({self.selected_x}-{self.selected_x+self.patch_size}, \"\n",
    "                      f\"{self.selected_y}-{self.selected_y+self.patch_size})\")\n",
    "        \n",
    "        # 2. Text with classification and VQA\n",
    "        context = f\"{zone}\\n\\nAlpha: {patch_alpha:.1f}Â°\\nEntropy: {patch_entropy:.2f}\\nAnisotropy: {patch_anisotropy:.2f}\"\n",
    "        if question:\n",
    "            answer = qa_pipeline(question=question, context=context)\n",
    "            context += f\"\\n\\nQ: {question}\\nA: {answer['answer']}\"\n",
    "        ax2.text(0.5, 0.5, context, ha='center', va='center', fontsize=14)\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # 3. Enhanced H/Alpha plot with zone boundaries\n",
    "        ax3.scatter(patch_alpha, patch_entropy, c='red', s=150)\n",
    "        ax3.set_xlim(0, 90)\n",
    "        ax3.set_ylim(0, 1)\n",
    "        ax3.set_xlabel(\"Alpha Angle (Â°)\", fontsize=12)\n",
    "        ax3.set_ylabel(\"Entropy\", fontsize=12)\n",
    "        ax3.axhline(0.3, color='red', linestyle='--', alpha=0.5)\n",
    "        ax3.axhline(0.7, color='red', linestyle='--', alpha=0.5)\n",
    "        ax3.axvline(30, color='blue', linestyle='--', alpha=0.5)\n",
    "        ax3.axvline(50, color='blue', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add zone labels\n",
    "        ax3.text(15, 0.15, \"Z1\", fontsize=10)\n",
    "        ax3.text(40, 0.15, \"Z2\", fontsize=10)\n",
    "        ax3.text(70, 0.15, \"Z3\", fontsize=10)\n",
    "        ax3.text(15, 0.5, \"Z4\", fontsize=10)\n",
    "        ax3.text(40, 0.5, \"Z5\", fontsize=10)\n",
    "        ax3.text(70, 0.5, \"Z6\", fontsize=10)\n",
    "        ax3.text(15, 0.85, \"Z7\", fontsize=10)\n",
    "        ax3.text(40, 0.85, \"Z8\", fontsize=10)\n",
    "        ax3.text(70, 0.85, \"Z9\", fontsize=10)\n",
    "        \n",
    "        ax3.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = SARImageAnalyzer()\n",
    "\n",
    "# Update analysis when question changes (without requiring a new click)\n",
    "def on_question_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        with analysis_output:\n",
    "            clear_output(wait=True)\n",
    "            analyzer.analyze_region(change['new'])\n",
    "\n",
    "question_input.observe(on_question_change, names='value')\n",
    "\n",
    "# Display widgets\n",
    "display(question_input, analysis_output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
